{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## ::: Scan input fasta and make predictions ::: ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import pybedtools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False # Supress tensorflow warning\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Supress tensorflow warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.dirname(os.path.realpath('scan.ipynb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_seq(s):\n",
    "    ns = s.upper()    \n",
    "    pattern = re.compile(r'\\s+')\n",
    "    ns = re.sub(pattern, '', ns)\n",
    "    ns = re.sub(r'[^a-zA-Z]{1}', 'N', ns)\n",
    "    return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in fasta\n",
    "fasta_dir = dir_path + \"/../data/genome/GRCh38.chr1to22.fa\"\n",
    "fasta = SeqIO.to_dict(SeqIO.parse(open(fasta_dir),'fasta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert seq to str and clean seq\n",
    "fasta = {k: clean_seq(str(v.seq)) for k,v in fasta.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(seq, strand):\n",
    "    enc_mat = np.append(np.eye(4), [[0,0,0,0]], axis=0)\n",
    "    enc_mat = enc_mat.astype(np.bool)\n",
    "    mapping_pos = dict(zip(\"ACGTN\", range(5)))\n",
    "    mapping_neg = dict(zip(\"TGCAN\", range(5)))\n",
    "    \n",
    "    if(strand == \"+\"):\n",
    "        seq2 = [mapping_pos[i] for i in seq]\n",
    "    else:\n",
    "        seq = seq[::-1]\n",
    "        seq2 = [mapping_neg[i] for i in seq]\n",
    "    return enc_mat[seq2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_promid(idx, seq, half_size, key, strand):\n",
    "    subseq = seq[idx - half_size: idx + half_size + 1]\n",
    "    \n",
    "    # make index\n",
    "    if strand == \"-\":\n",
    "        idx1 = len(seq) - (idx + half_size) + 1\n",
    "        idx2 = len(seq) - (idx - half_size) + 1\n",
    "    else:\n",
    "        idx1 = idx - half_size + 1\n",
    "        idx2 = idx + half_size + 1\n",
    "        \n",
    "    ck = \"{}:{}-{} {}\".format(key, idx1, idx2, strand)\n",
    "    \n",
    "    if not 'N' in subseq:\n",
    "        return (ck, subseq)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(batch, sess):\n",
    "#     return sess.run(y, feed_dict={input_x: batch, kr: 1.0, in_training_mode: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  28\n",
      "* Processing chr22\n",
      "  Positive strand\n",
      "\t - Scanning... Done! Number of subsequences: 391140\n",
      "\t - Starting Tensorflow... loading saved model... loaded!\n",
      "\t - One-hot encoding scanned subsequences... Done!\n",
      "\t - Begin prediction\n",
      "\t\tbatch:  100 out of  3056\n",
      "\t\tbatch:  200 out of  3056\n",
      "\t\tbatch:  300 out of  3056\n",
      "\t\tbatch:  400 out of  3056\n",
      "\t\tbatch:  500 out of  3056\n",
      "\t\tbatch:  600 out of  3056\n",
      "\t\tbatch:  700 out of  3056\n",
      "\t\tbatch:  800 out of  3056\n",
      "\t\tbatch:  900 out of  3056\n",
      "\t\tbatch:  1000 out of  3056\n",
      "\t\tbatch:  1100 out of  3056\n",
      "\t\tbatch:  1200 out of  3056\n",
      "\t\tbatch:  1300 out of  3056\n",
      "\t\tbatch:  1400 out of  3056\n",
      "\t\tbatch:  1500 out of  3056\n",
      "\t\tbatch:  1600 out of  3056\n",
      "\t\tbatch:  1700 out of  3056\n",
      "\t\tbatch:  1800 out of  3056\n",
      "\t\tbatch:  1900 out of  3056\n",
      "\t\tbatch:  2000 out of  3056\n",
      "\t\tbatch:  2100 out of  3056\n",
      "\t\tbatch:  2200 out of  3056\n",
      "\t\tbatch:  2300 out of  3056\n",
      "\t\tbatch:  2400 out of  3056\n",
      "\t\tbatch:  2500 out of  3056\n",
      "\t\tbatch:  2600 out of  3056\n",
      "\t\tbatch:  2700 out of  3056\n",
      "\t\tbatch:  2800 out of  3056\n",
      "\t\tbatch:  2900 out of  3056\n",
      "\t\tbatch:  3000 out of  3056\n",
      "\t Prediction finished!\n",
      "  Negative strand\n",
      "\t - Scanning... Done! Number of subsequences: 391143\n",
      "\t - Starting Tensorflow... loading saved model... loaded!\n",
      "\t - One-hot encoding scanned subsequences... Done!\n",
      "\t - Begin prediction\n",
      "\t\tbatch:  100 out of  3056\n",
      "\t\tbatch:  200 out of  3056\n",
      "\t\tbatch:  300 out of  3056\n",
      "\t\tbatch:  400 out of  3056\n",
      "\t\tbatch:  500 out of  3056\n",
      "\t\tbatch:  600 out of  3056\n",
      "\t\tbatch:  700 out of  3056\n",
      "\t\tbatch:  800 out of  3056\n",
      "\t\tbatch:  900 out of  3056\n",
      "\t\tbatch:  1000 out of  3056\n",
      "\t\tbatch:  1100 out of  3056\n",
      "\t\tbatch:  1200 out of  3056\n",
      "\t\tbatch:  1300 out of  3056\n",
      "\t\tbatch:  1400 out of  3056\n",
      "\t\tbatch:  1500 out of  3056\n",
      "\t\tbatch:  1600 out of  3056\n",
      "\t\tbatch:  1700 out of  3056\n",
      "\t\tbatch:  1800 out of  3056\n",
      "\t\tbatch:  1900 out of  3056\n",
      "\t\tbatch:  2000 out of  3056\n",
      "\t\tbatch:  2100 out of  3056\n",
      "\t\tbatch:  2200 out of  3056\n",
      "\t\tbatch:  2300 out of  3056\n",
      "\t\tbatch:  2400 out of  3056\n",
      "\t\tbatch:  2500 out of  3056\n",
      "\t\tbatch:  2600 out of  3056\n",
      "\t\tbatch:  2700 out of  3056\n",
      "\t\tbatch:  2800 out of  3056\n",
      "\t\tbatch:  2900 out of  3056\n",
      "\t\tbatch:  3000 out of  3056\n",
      "\t Prediction finished!\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing\n",
    "print(\"Number of processors: \", mp.cpu_count())\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# globals\n",
    "scan_step = 100\n",
    "half_size = 500\n",
    "batch_size = 128\n",
    "start = half_size\n",
    "cutoff = 0.5\n",
    "proms = {}\n",
    "scores = {}\n",
    "\n",
    "for key in fasta.keys():\n",
    "    print(\"* Processing \" + key)\n",
    "    for strand in [\"+\", \"-\"]:\n",
    "        print(\"  {} strand\".format(\"Positive\" if strand == \"+\" else \"Negative\"))\n",
    "        seq = fasta[key]\n",
    "        if strand == \"-\": \n",
    "            seq = seq[::-1]\n",
    "#         putative = [pool.apply(scan_promid, args=(i, seq, half_size, key, strand)) for i in range(start, len(seq) - half_size, scan_step)]\n",
    "#         putative = pool.map(scan_promid, range(start, len(seq) - half_size, scan_step)) # multiprocessing\n",
    "        print(\"\\t - Scanning...\", end = \" \")\n",
    "        putative = pool.starmap(scan_promid, [(i, seq, half_size, key, strand) for i in range(start, len(seq) - half_size, scan_step)]) # multiprocessing\n",
    "        putative = list(filter(None,putative)) # filter subseq with N\n",
    "        proms.update({k: v for (k,v) in putative}) # store in dict\n",
    "        coords = [k for (k,v) in putative] # store coordinates separately\n",
    "        putative = [v for (k,v) in putative] # use only seq to predict\n",
    "        probs = []\n",
    "        print(\"Done! Number of subsequences: {}\".format(len(putative)))\n",
    "        ######### predict here, use putative #########\n",
    "        new_graph = tf.Graph()\n",
    "        print(\"\\t - Starting Tensorflow...\", end = \" \")\n",
    "        with tf.Session(graph=new_graph) as sess:\n",
    "            print(\"loading saved model...\", end = \" \")\n",
    "            tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], \"/projects/b1017/Jerry/PromID/promid/models/model_scan\")\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, \"/projects/b1017/Jerry/PromID/promid/models/model_scan/variables/variables\")\n",
    "            input_x = tf.get_default_graph().get_tensor_by_name(\"input_prom:0\")\n",
    "            y = tf.get_default_graph().get_tensor_by_name(\"output_prom:0\")\n",
    "            kr = tf.get_default_graph().get_tensor_by_name(\"kr:0\")\n",
    "            in_training_mode = tf.get_default_graph().get_tensor_by_name(\"in_training_mode:0\")  \n",
    "            print(\"loaded!\")\n",
    "            \n",
    "            # predict\n",
    "            print(\"\\t - One-hot encoding scanned subsequences...\", end = \" \")\n",
    "            encoded = [encode(fa, strand) for fa in putative]\n",
    "            print(\"Done!\")\n",
    "#             test_res = pool.starmap(predict, [(batch, sess) for batch in chunks(encoded, batch_size)])\n",
    "            print(\"\\t - Begin prediction\")\n",
    "            i = 1\n",
    "            for batch in chunks(encoded, batch_size):\n",
    "                if i % 100 == 0:\n",
    "                    print(\"\\t\\tbatch: \", i, \"out of \", math.ceil(len(encoded)/batch_size))\n",
    "                pred = sess.run(y, feed_dict={input_x: batch, kr: 1.0, in_training_mode: False})\n",
    "                probs.extend([prob[0] for prob in pred])\n",
    "                i += 1\n",
    "            print(\"\\t Prediction finished!\")\n",
    "                \n",
    "        ######### output is a list: probs #########\n",
    "        scores.update({coords[i]: probs[i] for i in range(len(probs)) if probs[i] > cutoff}) # filter\n",
    "\n",
    "        \n",
    "# filter\n",
    "proms = {k: v for k,v in proms.items() if k in scores.keys()}\n",
    "        \n",
    "        \n",
    "pool.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_path + \"/../results/benchmark/promid/GRCh38.chr22.proms.pkl\", 'wb') as f:\n",
    "    pickle.dump(proms, f)\n",
    "\n",
    "# with open(dir_path + \"/../results/benchmark/promid/GRCh38.chr22.proms.all.pkl\", 'wb') as f:\n",
    "#     pickle.dump(proms_all, f)\n",
    "\n",
    "with open(dir_path + \"/../results/benchmark/promid/GRCh38.chr22.scores.pkl\", 'wb') as f:\n",
    "    pickle.dump(scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.load(dir_path + \"/../results/benchmark/promid/GRCh38.chr1to22.proms.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example bed\n",
    "# for i, x in enumerate(list(scores)[1:100]):\n",
    "#     delim = re.split('[^a-zA-Z0-9+-]',x)\n",
    "#     coords = re.split('-',delim[1])\n",
    "#     print(\"{}\\t{}\\t{}\\tprom{}\\t0\\t{}\".format(delim[0],coords[0],coords[1],i,delim[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bed file of the positive predictions\n",
    "bed = []\n",
    "for i, x in enumerate(list(proms)):\n",
    "    delim = re.split('[^a-zA-Z0-9+-]',x)\n",
    "    coords = re.split('-',delim[1])\n",
    "    bed.append(\"{}\\t{}\\t{}\\tprom{}\\t0\\t{}\".format(delim[0],coords[0],coords[1],i,delim[2]))\n",
    "with open(dir_path + \"/../results/benchmark/promid/GRCh38.chr22.proms.bed\", 'w+') as f:\n",
    "    f.write('\\n'.join(bed))\n",
    "    \n",
    "# # create bed file of all predictions\n",
    "# bed_all = []\n",
    "# for i, x in enumerate(list(proms_all)):\n",
    "#     delim = re.split('[^a-zA-Z0-9+-]',x)\n",
    "#     coords = re.split('-',delim[1])\n",
    "#     bed_all.append(\"{}\\t{}\\t{}\\tprom{}\\t0\\t{}\".format(delim[0],coords[0],coords[1],i,delim[2]))\n",
    "# with open(dir_path + \"/../results/benchmark/promid/GRCh38.chr22.proms.all.bed\", 'w+') as f:\n",
    "#     f.write('\\n'.join(bed_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### pybedtools #####\n",
    "# read in true TSS bed file, and make true promoter file -500 to 500 \n",
    "TSS_bed = pybedtools.BedTool(dir_path + \"/../data/promoter/human_epdnew_hg38.bed\")\n",
    "true_prom_bed = TSS_bed.slop(g=dir_path + \"/../data/genome/chrom.sizes\",l=499,r=500) # -500 to 500\n",
    "\n",
    "# read in predicted promoter bed files\n",
    "prom_bed = pybedtools.BedTool(dir_path + \"/../results/benchmark/promid/GRCh38.chr1to22.proms.bed\")\n",
    "# all_prom_bed = pybedtools.BedTool(dir_path + \"/../results/benchmark/promid/GRCh38.chr22.proms.all.bed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersect\n",
    "# TP\n",
    "overlap_TP = prom_bed.intersect(true_prom_bed,\n",
    "#                                 wo=True, # Write the original entry in A for each overlap\n",
    "#                                 c=True, # For each entry in A, report the number of overlaps with B\n",
    "                                s=True, # Require same strandedness.  That is, only report hits in that overlap A on the **same** strand.\n",
    "                                f=0.5, # Minimum overlap required as a fraction of A\n",
    "                                u=True, # Write the original A entry **once** if **any** overlaps found in B\n",
    "                                r=True # Require that the fraction overlap be reciprocal for A AND B\n",
    "                                )\n",
    "\n",
    "# FP\n",
    "overlap_FP = prom_bed.intersect(true_prom_bed, \n",
    "                                v=True, # Only report those entries in A that have **no overlaps** with B.\n",
    "                                s=True, # Require same strandedness.  That is, only report hits in that overlap A on the **same** strand.\n",
    "                                f=0.5, # Minimum overlap required as a fraction of A\n",
    "#                                 u=True, # Write the original A entry **once** if **any** overlaps found in B\n",
    "                                r=True # Require that the fraction overlap be reciprocal for A AND B\n",
    "                                )\n",
    "\n",
    "# FN\n",
    "overlap_FN = true_prom_bed.intersect(prom_bed,\n",
    "                                     v=True, # Only report those entries in A that have **no overlaps** with B.\n",
    "                                     s=True, # Require same strandedness.  That is, only report hits in that overlap A on the **same** strand.\n",
    "                                     f=0.5, # Minimum overlap required as a fraction of A\n",
    "#                                      u=True, # Write the original A entry **once** if **any** overlaps found in B\n",
    "                                     r=True # Require that the fraction overlap be reciprocal for A AND B\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "TP = len(overlap_TP)\n",
    "FP = len(overlap_FP)\n",
    "FN = len(overlap_FN)*10\n",
    "\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "F1 = 2*(precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision =  0.020940875941382314\n",
      "recall =  0.6489957346026052\n",
      "F1 =  0.040572612246892854\n"
     ]
    }
   ],
   "source": [
    "print(\"precision = \", precision)\n",
    "print(\"recall = \", recall)\n",
    "print(\"F1 = \", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112898 5278376 61060\n"
     ]
    }
   ],
   "source": [
    "print(TP, FP, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
